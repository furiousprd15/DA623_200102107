# DA623_200102107
# PCA vs Autoencoder: A Comparative Analysis

This repository contains a Jupyter notebook comparing Principal Component Analysis (PCA) and autoencoders using the MNIST dataset.

## Overview

- PCA and autoencoders are both dimensionality reduction techniques used in machine learning and data analysis.
- This notebook aims to:
  - Introduce PCA and autoencoders.
  - Implement PCA and an autoencoder using Python.
  - Compare and contrast the performance and characteristics of PCA and autoencoders.
- The MNIST dataset, consisting of handwritten digit images, is used for demonstration.

## Contents

1. Introduction
2. Data Import and Exploration
3. The MNIST Dataset
4. Data Exploration
5. Principal Component Analysis (PCA)
   - PCA with different PC
   - Visualization of reconstruction
   - Effect of Variance threshold (PC component) on reconstruction error
   - Reconstruction at 90% variance
   - PCA with scikit-learn
6. Basis Functions
7. Image reconstruction trend with increasing number of components
8. Classification Capacity
9. Autoencoders for Data Reduction
   - Model Architecture
   - Model Training
   - Autoencoder representation
   - Stability of dimension reduction using noise suppression
   - Noise Suppression in Autoencoder
10. Summary

## References

- [How Do PCA and Autoencoders Compare in Dimensionality Reduction?](https://www.linkedin.com/advice/1/how-do-pca-autoencoders-compare-dimensionality-osatc#:~:text=PCA%20is%20a%20linear%20technique,but%20computationally%20intensive%20and%20opaque.)
- [PCA in Python](https://builtin.com/machine-learning/pca-in-python)

## Getting Started

To run the notebook, follow these steps:

1. Clone this repository:

   ```bash
   git clone https://github.com/furiousprd15/DA623_200102107.git
